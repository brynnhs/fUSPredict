{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "d7b1f0c9",
            "metadata": {},
            "source": [
                "## dataset creation\n",
                "adapted from Leo's code\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "37da4888",
            "metadata": {},
            "outputs": [],
            "source": [
                "from pathlib import Path\n",
                "import sys\n",
                "import os\n",
                "import glob\n",
                "import random\n",
                "import json\n",
                "from collections import Counter, OrderedDict\n",
                "from datetime import datetime\n",
                "import numpy as np\n",
                "import torch\n",
                "from torch.utils.data import Dataset\n",
                "repo_root = Path.cwd().parents[0]\n",
                "sys.path.insert(0, str(repo_root))\n",
                "sys.path.insert(0, str(repo_root / \"code\"))\n",
                "from utils import helper_functions as hf\n",
                "from collections import Counter, OrderedDict\n",
                "from datetime import datetime\n",
                "import json"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "dfca860c",
            "metadata": {},
            "source": [
                "## Part 1: discovery + split-manifest utilities (single-subject, multi-subject, held-out)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "58b7fdbc",
            "metadata": {},
            "outputs": [],
            "source": [
                "def discover_acquisition_npz(cache_dir, pattern=\"baseline_*.npz\"):\n",
                "    \"\"\"Find one cached .npz per acquisition in one cache dir.\"\"\"\n",
                "    paths = sorted(glob.glob(os.path.join(str(cache_dir), pattern)))\n",
                "    if not paths:\n",
                "        raise FileNotFoundError(f\"No acquisition caches found in {cache_dir} with pattern {pattern}\")\n",
                "    return [str(p) for p in paths]\n",
                "def discover_acquisitions_multi(subject_to_cache_dir, pattern=\"baseline_*.npz\"):\n",
                "    \"\"\"\n",
                "    Discover acquisitions across subjects.\n",
                "    Returns deterministic list: [{\"path\": str, \"subject\": str}, ...]\n",
                "    \"\"\"\n",
                "    if not isinstance(subject_to_cache_dir, dict) or len(subject_to_cache_dir) == 0:\n",
                "        raise ValueError(\"subject_to_cache_dir must be a non-empty dict\")\n",
                "    out = []\n",
                "    for subject in sorted(subject_to_cache_dir.keys()):\n",
                "        for p in discover_acquisition_npz(subject_to_cache_dir[subject], pattern=pattern):\n",
                "            out.append({\"path\": p, \"subject\": str(subject)})\n",
                "    return out\n",
                "def _read_num_frames(npz_path, frames_key=\"frames\"):\n",
                "    with np.load(npz_path, allow_pickle=True) as z:\n",
                "        if frames_key not in z:\n",
                "            raise KeyError(f\"{npz_path} missing key '{frames_key}'\")\n",
                "        x = z[frames_key]\n",
                "        if x.ndim not in (3, 4):\n",
                "            raise ValueError(f\"{npz_path} '{frames_key}' must be [T,H,W] or [T,C,H,W], got {x.shape}\")\n",
                "        return int(x.shape[0])\n",
                "def _count_by_subject(paths, path_to_subject):\n",
                "    return Counter(path_to_subject.get(p, \"UNK\") for p in paths)\n",
                "def _print_split_summary(train_paths, test_paths, path_to_subject, frames_key=\"frames\"):\n",
                "    train_frames = sum(_read_num_frames(p, frames_key=frames_key) for p in train_paths)\n",
                "    test_frames = sum(_read_num_frames(p, frames_key=frames_key) for p in test_paths)\n",
                "    tr_cnt = _count_by_subject(train_paths, path_to_subject)\n",
                "    te_cnt = _count_by_subject(test_paths, path_to_subject)\n",
                "    tr_txt = \", \".join([f\"{m}:{tr_cnt[m]}\" for m in sorted(tr_cnt.keys())])\n",
                "    te_txt = \", \".join([f\"{m}:{te_cnt[m]}\" for m in sorted(te_cnt.keys())])\n",
                "    print(f\"Train acquisitions: {len(train_paths)} | total frames: {train_frames} | per-subject=({tr_txt})\")\n",
                "    print(f\"Test acquisitions:  {len(test_paths)} | total frames: {test_frames} | per-subject=({te_txt})\")\n",
                "def _write_manifest(manifest, manifest_path):\n",
                "    os.makedirs(os.path.dirname(str(manifest_path)), exist_ok=True)\n",
                "    with open(manifest_path, \"w\", encoding=\"utf-8\") as f:\n",
                "        json.dump(manifest, f, indent=2)\n",
                "    print(f\"Manifest saved: {manifest_path}\")\n",
                "    return str(manifest_path)\n",
                "def create_split_manifest(\n",
                "    cache_dir,\n",
                "    split_ratio=0.8,\n",
                "    seed=42,\n",
                "    manifest_path=None,\n",
                "    pattern=\"baseline_*.npz\",\n",
                "    frames_key=\"frames\",\n",
                "):\n",
                "    \"\"\"Single-cache deterministic acquisition-wise split.\"\"\"\n",
                "    if not (0.0 < split_ratio < 1.0):\n",
                "        raise ValueError(\"split_ratio must be between 0 and 1\")\n",
                "    paths = discover_acquisition_npz(cache_dir, pattern=pattern)\n",
                "    rng = random.Random(seed)\n",
                "    shuffled = paths.copy()\n",
                "    rng.shuffle(shuffled)\n",
                "    n_total = len(shuffled)\n",
                "    n_train = int(split_ratio * n_total)\n",
                "    n_train = max(1, min(n_total - 1, n_train)) if n_total > 1 else 1\n",
                "    train_paths = sorted(shuffled[:n_train])\n",
                "    test_paths = sorted(shuffled[n_train:])\n",
                "    manifest = {\n",
                "        \"train\": train_paths,\n",
                "        \"test\": test_paths,\n",
                "        \"seed\": int(seed),\n",
                "        \"split_ratio\": float(split_ratio),\n",
                "        \"timestamp\": datetime.now().isoformat(timespec=\"seconds\"),\n",
                "        \"cache_dir\": str(cache_dir),\n",
                "        \"pattern\": pattern,\n",
                "        \"frames_key\": frames_key,\n",
                "    }\n",
                "    if manifest_path is None:\n",
                "        manifest_path = os.path.join(str(cache_dir), \"splits.json\")\n",
                "    out = _write_manifest(manifest, manifest_path)\n",
                "    path_to_subject = {p: Path(p).parent.name for p in train_paths + test_paths}\n",
                "    _print_split_summary(train_paths, test_paths, path_to_subject, frames_key=frames_key)\n",
                "    return out\n",
                "def create_split_manifest_multi(\n",
                "    subject_to_cache_dir,\n",
                "    split_ratio=0.8,\n",
                "    seed=42,\n",
                "    pattern=\"baseline_*.npz\",\n",
                "    frames_key=\"frames\",\n",
                "    manifest_path=None,\n",
                "):\n",
                "    \"\"\"Combined split across all subjects (acquisition-wise random, subject metadata preserved).\"\"\"\n",
                "    if not (0.0 < split_ratio < 1.0):\n",
                "        raise ValueError(\"split_ratio must be between 0 and 1\")\n",
                "    acqs = discover_acquisitions_multi(subject_to_cache_dir, pattern=pattern)\n",
                "    rng = random.Random(seed)\n",
                "    shuffled = acqs.copy()\n",
                "    rng.shuffle(shuffled)\n",
                "    n_total = len(shuffled)\n",
                "    n_train = int(split_ratio * n_total)\n",
                "    n_train = max(1, min(n_total - 1, n_train)) if n_total > 1 else 1\n",
                "    train_acqs = sorted(shuffled[:n_train], key=lambda d: d[\"path\"])\n",
                "    test_acqs = sorted(shuffled[n_train:], key=lambda d: d[\"path\"])\n",
                "    train_paths = [d[\"path\"] for d in train_acqs]\n",
                "    test_paths = [d[\"path\"] for d in test_acqs]\n",
                "    path_to_subject = {d[\"path\"]: d[\"subject\"] for d in acqs}\n",
                "    manifest = {\n",
                "        \"train\": train_paths,\n",
                "        \"test\": test_paths,\n",
                "        \"meta\": {p: {\"subject\": m} for p, m in path_to_subject.items()},\n",
                "        \"acquisitions\": acqs,\n",
                "        \"seed\": int(seed),\n",
                "        \"split_ratio\": float(split_ratio),\n",
                "        \"timestamp\": datetime.now().isoformat(timespec=\"seconds\"),\n",
                "        \"subject_to_cache_dir\": {str(k): str(v) for k, v in subject_to_cache_dir.items()},\n",
                "        \"pattern\": pattern,\n",
                "        \"frames_key\": frames_key,\n",
                "    }\n",
                "    if manifest_path is None:\n",
                "        first_dir = str(subject_to_cache_dir[sorted(subject_to_cache_dir.keys())[0]])\n",
                "        manifest_path = os.path.join(first_dir, \"splits_multi.json\")\n",
                "    out = _write_manifest(manifest, manifest_path)\n",
                "    _print_split_summary(train_paths, test_paths, path_to_subject, frames_key=frames_key)\n",
                "    return out\n",
                "def create_subject_heldout_manifests(\n",
                "    subject_to_cache_dir,\n",
                "    pattern=\"baseline_*.npz\",\n",
                "    frames_key=\"frames\",\n",
                "    output_dir=None,\n",
                "):\n",
                "    \"\"\"\n",
                "    2-subject held-out split manifests:\n",
                "    - <A>_train_<B>_test.json\n",
                "    - <B>_train_<A>_test.json\n",
                "    \"\"\"\n",
                "    if not isinstance(subject_to_cache_dir, dict) or len(subject_to_cache_dir) != 2:\n",
                "        raise ValueError(\"subject_to_cache_dir must contain exactly 2 subjects\")\n",
                "    a, b = sorted(subject_to_cache_dir.keys())\n",
                "    a_acqs = discover_acquisitions_multi({a: subject_to_cache_dir[a]}, pattern=pattern)\n",
                "    b_acqs = discover_acquisitions_multi({b: subject_to_cache_dir[b]}, pattern=pattern)\n",
                "    a_paths = sorted([d[\"path\"] for d in a_acqs])\n",
                "    b_paths = sorted([d[\"path\"] for d in b_acqs])\n",
                "    path_to_subject = {d[\"path\"]: d[\"subject\"] for d in (a_acqs + b_acqs)}\n",
                "    meta = {p: {\"subject\": m} for p, m in path_to_subject.items()}\n",
                "    if output_dir is None:\n",
                "        output_dir = str(subject_to_cache_dir[a])\n",
                "    os.makedirs(output_dir, exist_ok=True)\n",
                "    m1_path = os.path.join(output_dir, f\"{a}_train_{b}_test.json\")\n",
                "    m2_path = os.path.join(output_dir, f\"{b}_train_{a}_test.json\")\n",
                "    m1 = {\n",
                "        \"train\": a_paths,\n",
                "        \"test\": b_paths,\n",
                "        \"meta\": meta,\n",
                "        \"acquisitions\": a_acqs + b_acqs,\n",
                "        \"heldout_mode\": f\"train={a},test={b}\",\n",
                "        \"timestamp\": datetime.now().isoformat(timespec=\"seconds\"),\n",
                "        \"pattern\": pattern,\n",
                "        \"frames_key\": frames_key,\n",
                "    }\n",
                "    m2 = {\n",
                "        \"train\": b_paths,\n",
                "        \"test\": a_paths,\n",
                "        \"meta\": meta,\n",
                "        \"acquisitions\": a_acqs + b_acqs,\n",
                "        \"heldout_mode\": f\"train={b},test={a}\",\n",
                "        \"timestamp\": datetime.now().isoformat(timespec=\"seconds\"),\n",
                "        \"pattern\": pattern,\n",
                "        \"frames_key\": frames_key,\n",
                "    }\n",
                "    out1 = _write_manifest(m1, m1_path)\n",
                "    print(f\"Summary for {Path(out1).name}\")\n",
                "    _print_split_summary(a_paths, b_paths, path_to_subject, frames_key=frames_key)\n",
                "    out2 = _write_manifest(m2, m2_path)\n",
                "    print(f\"Summary for {Path(out2).name}\")\n",
                "    _print_split_summary(b_paths, a_paths, path_to_subject, frames_key=frames_key)\n",
                "    return out1, out2\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "a9c76898",
            "metadata": {},
            "source": [
                "## per-acquisition forecasting Dataset\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "id": "4d905d84",
            "metadata": {},
            "outputs": [],
            "source": [
                "class FUSForecastWindowDataset(Dataset):\n",
                "    \"\"\"\n",
                "    Per-acquisition lazy sliding-window forecasting dataset.\n",
                "    Returns:\n",
                "      - context: [W, C, H, W]\n",
                "      - target:  [K, C, H, W]\n",
                "    If return_meta=True:\n",
                "      - (context, target, {\"subject\", \"path\", \"end_ctx\"})\n",
                "    \"\"\"\n",
                "    def __init__(\n",
                "        self,\n",
                "        manifest_path=None,\n",
                "        acq_paths=None,\n",
                "        split=None,\n",
                "        window_size=8,\n",
                "        pred_horizon=1,\n",
                "        stride=1,\n",
                "        frames_key=\"frames\",\n",
                "        labels_key=None,\n",
                "        mask_key=None,\n",
                "        exclude_label=-1,\n",
                "        lru_cache_size=2,\n",
                "        target_size=112,\n",
                "        return_meta=False,\n",
                "    ):\n",
                "        if window_size <= 0 or pred_horizon <= 0 or stride <= 0:\n",
                "            raise ValueError(\"window_size, pred_horizon, and stride must all be > 0\")\n",
                "        if manifest_path is None and acq_paths is None:\n",
                "            raise ValueError(\"Provide either manifest_path or acq_paths\")\n",
                "        self.window_size = int(window_size)\n",
                "        self.pred_horizon = int(pred_horizon)\n",
                "        self.stride = int(stride)\n",
                "        self.frames_key = frames_key\n",
                "        self.labels_key = labels_key\n",
                "        self.mask_key = mask_key\n",
                "        self.exclude_label = exclude_label\n",
                "        self.lru_cache_size = int(max(1, lru_cache_size))\n",
                "        self.target_size = int(target_size)\n",
                "        self.return_meta = bool(return_meta)\n",
                "        self.acq_paths, self.acq_subjects = self._resolve_inputs(manifest_path, acq_paths, split)\n",
                "        for p in self.acq_paths:\n",
                "            if not os.path.exists(p):\n",
                "                raise FileNotFoundError(f\"Missing acquisition file: {p}\")\n",
                "        self._acq_cache = OrderedDict()\n",
                "        self.index_map = []\n",
                "        self.acq_meta = []\n",
                "        self.expected_chw = None\n",
                "        self._build_index_map()\n",
                "        if len(self.index_map) == 0:\n",
                "            raise RuntimeError(\"No valid windows found. Check window/pred/stride and exclusion rules.\")\n",
                "    def _resolve_inputs(self, manifest_path, acq_paths, split):\n",
                "        path_to_subject = {}\n",
                "        if manifest_path is not None:\n",
                "            with open(manifest_path, \"r\", encoding=\"utf-8\") as f:\n",
                "                m = json.load(f)\n",
                "            # Optional metadata map: meta[path] -> {subject: ...}\n",
                "            meta = m.get(\"meta\", {})\n",
                "            if isinstance(meta, dict):\n",
                "                for p, info in meta.items():\n",
                "                    if isinstance(info, dict) and info.get(\"subject\") is not None:\n",
                "                        path_to_subject[str(p)] = str(info[\"subject\"])\n",
                "            # Optional acquisitions list: [{path, subject}, ...]\n",
                "            acqs = m.get(\"acquisitions\", [])\n",
                "            if isinstance(acqs, list):\n",
                "                for item in acqs:\n",
                "                    if isinstance(item, dict) and \"path\" in item and \"subject\" in item:\n",
                "                        path_to_subject[str(item[\"path\"])] = str(item[\"subject\"])\n",
                "            if split is None:\n",
                "                paths = list(m.get(\"train\", [])) + list(m.get(\"test\", []))\n",
                "            else:\n",
                "                if split not in (\"train\", \"test\"):\n",
                "                    raise ValueError(\"split must be 'train' or 'test'\")\n",
                "                paths = list(m.get(split, []))\n",
                "        else:\n",
                "            paths = list(acq_paths)\n",
                "        if not paths:\n",
                "            raise ValueError(\"No acquisition files provided\")\n",
                "        paths = [str(p) for p in paths]\n",
                "        subjects = [path_to_subject.get(p, Path(p).parent.name) for p in paths]\n",
                "        return paths, subjects\n",
                "    def _get_invalid_mask(self, z, Tn, path):\n",
                "        invalid = np.zeros(Tn, dtype=bool)\n",
                "        if self.labels_key is not None and self.labels_key in z:\n",
                "            labels = np.asarray(z[self.labels_key]).squeeze()\n",
                "            if labels.shape[0] != Tn:\n",
                "                raise ValueError(f\"{path}: labels length {labels.shape[0]} != T {Tn}\")\n",
                "            invalid |= (labels == self.exclude_label)\n",
                "        if self.mask_key is not None and self.mask_key in z:\n",
                "            mask = np.asarray(z[self.mask_key]).squeeze()\n",
                "            if mask.shape[0] != Tn:\n",
                "                raise ValueError(f\"{path}: mask length {mask.shape[0]} != T {Tn}\")\n",
                "            invalid |= mask.astype(bool)\n",
                "        return invalid\n",
                "    def _normalize_frames(self, frames, path):\n",
                "        if frames.ndim == 3:\n",
                "            frames = frames[:, np.newaxis, :, :]\n",
                "        if frames.ndim != 4:\n",
                "            raise ValueError(f\"{path}: frames must be [T,H,W] or [T,C,H,W], got {frames.shape}\")\n",
                "        if frames.shape[1] != 1:\n",
                "            raise ValueError(f\"{path}: np_pad_or_crop_to_square expects channel=1, got C={frames.shape[1]}\")\n",
                "        frames = hf.np_pad_or_crop_to_square(frames, target_size=self.target_size).astype(np.float32, copy=False)\n",
                "        return frames\n",
                "    def _build_index_map(self):\n",
                "        for acq_idx, path in enumerate(self.acq_paths):\n",
                "            with np.load(path, allow_pickle=True) as z:\n",
                "                if self.frames_key not in z:\n",
                "                    raise KeyError(f\"{path} missing frames key '{self.frames_key}'\")\n",
                "                frames = self._normalize_frames(z[self.frames_key], path)\n",
                "                Tn, Cn, Hn, Wn = frames.shape\n",
                "                if self.expected_chw is None:\n",
                "                    self.expected_chw = (Cn, Hn, Wn)\n",
                "                elif self.expected_chw != (Cn, Hn, Wn):\n",
                "                    raise ValueError(\n",
                "                        f\"Inconsistent frame shape across acquisitions: expected {self.expected_chw}, got {(Cn, Hn, Wn)} in {path}\"\n",
                "                    )\n",
                "                if Tn < self.window_size + self.pred_horizon:\n",
                "                    self.acq_meta.append({\"T\": Tn, \"shape\": (Cn, Hn, Wn), \"n_windows\": 0})\n",
                "                    continue\n",
                "                invalid = self._get_invalid_mask(z, Tn, path)\n",
                "                n_windows = 0\n",
                "                for end_ctx in range(self.window_size - 1, Tn - self.pred_horizon, self.stride):\n",
                "                    start_ctx = end_ctx - self.window_size + 1\n",
                "                    target_end = end_ctx + self.pred_horizon\n",
                "                    if start_ctx < 0 or target_end >= Tn:\n",
                "                        continue\n",
                "                    if invalid[start_ctx: target_end + 1].any():\n",
                "                        continue\n",
                "                    self.index_map.append((acq_idx, end_ctx))\n",
                "                    n_windows += 1\n",
                "                self.acq_meta.append({\"T\": Tn, \"shape\": (Cn, Hn, Wn), \"n_windows\": n_windows})\n",
                "    def __len__(self):\n",
                "        return len(self.index_map)\n",
                "    def _load_acquisition(self, acq_idx):\n",
                "        if acq_idx in self._acq_cache:\n",
                "            x = self._acq_cache.pop(acq_idx)\n",
                "            self._acq_cache[acq_idx] = x\n",
                "            return x\n",
                "        path = self.acq_paths[acq_idx]\n",
                "        with np.load(path, allow_pickle=True) as z:\n",
                "            frames = self._normalize_frames(z[self.frames_key], path)\n",
                "        _, Cn, Hn, Wn = frames.shape\n",
                "        if self.expected_chw != (Cn, Hn, Wn):\n",
                "            raise ValueError(f\"{path}: shape changed vs expected {self.expected_chw}, got {(Cn, Hn, Wn)}\")\n",
                "        self._acq_cache[acq_idx] = frames\n",
                "        if len(self._acq_cache) > self.lru_cache_size:\n",
                "            self._acq_cache.popitem(last=False)\n",
                "        return frames\n",
                "    def __getitem__(self, idx):\n",
                "        acq_idx, end_ctx = self.index_map[idx]\n",
                "        frames = self._load_acquisition(acq_idx)\n",
                "        start_ctx = end_ctx - self.window_size + 1\n",
                "        target_start = end_ctx + 1\n",
                "        target_end = end_ctx + self.pred_horizon\n",
                "        if start_ctx < 0 or target_end >= frames.shape[0]:\n",
                "            raise IndexError(\"Computed window is out of bounds\")\n",
                "        context = frames[start_ctx:end_ctx + 1]            # [W, C, H, W]\n",
                "        target = frames[target_start:target_end + 1]      # [K, C, H, W]\n",
                "        if context.shape[0] != self.window_size:\n",
                "            raise RuntimeError(f\"Context length mismatch: expected {self.window_size}, got {context.shape[0]}\")\n",
                "        if target.shape[0] != self.pred_horizon:\n",
                "            raise RuntimeError(f\"Target length mismatch: expected {self.pred_horizon}, got {target.shape[0]}\")\n",
                "        x = torch.from_numpy(context)\n",
                "        y = torch.from_numpy(target)\n",
                "        if x.dtype != torch.float32 or y.dtype != torch.float32:\n",
                "            raise TypeError(\"Dataset outputs must be float32 tensors\")\n",
                "        if self.return_meta:\n",
                "            info = {\n",
                "                \"subject\": self.acq_subjects[acq_idx],\n",
                "                \"path\": self.acq_paths[acq_idx],\n",
                "                \"end_ctx\": int(end_ctx),\n",
                "            }\n",
                "            return x, y, info\n",
                "        return x, y\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "5d92cd05",
            "metadata": {},
            "source": [
                "## use it\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "id": "18cbabdc",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Manifest saved: c:\\Users\\ESPCI\\Documents\\GitHub\\fUSPredict\\derivatives\\preprocessing\\splits_multi.json\n",
                        "Train acquisitions: 37 | total frames: 15518 | per-subject=(A:26, B:11)\n",
                        "Test acquisitions:  10 | total frames: 4363 | per-subject=(A:8, B:2)\n",
                        "Function pad shape:  (534, 1, 112, 112)\n",
                        "Function pad shape:  (549, 1, 112, 112)\n",
                        "Function pad shape:  (507, 1, 112, 112)\n",
                        "Function pad shape:  (268, 1, 112, 112)\n",
                        "Function pad shape:  (516, 1, 112, 112)\n",
                        "Function pad shape:  (539, 1, 112, 112)\n",
                        "Function pad shape:  (530, 1, 112, 112)\n",
                        "Function pad shape:  (528, 1, 112, 112)\n",
                        "Function pad shape:  (264, 1, 112, 112)\n",
                        "Function pad shape:  (269, 1, 112, 112)\n",
                        "Function pad shape:  (258, 1, 112, 112)\n",
                        "Function pad shape:  (526, 1, 112, 112)\n",
                        "Function pad shape:  (502, 1, 112, 112)\n",
                        "Function pad shape:  (538, 1, 112, 112)\n",
                        "Function pad shape:  (537, 1, 112, 112)\n",
                        "Function pad shape:  (252, 1, 112, 112)\n",
                        "Function pad shape:  (518, 1, 112, 112)\n",
                        "Function pad shape:  (509, 1, 112, 112)\n",
                        "Function pad shape:  (213, 1, 112, 112)\n",
                        "Function pad shape:  (513, 1, 112, 112)\n",
                        "Function pad shape:  (151, 1, 112, 112)\n",
                        "Function pad shape:  (542, 1, 112, 112)\n",
                        "Function pad shape:  (763, 1, 112, 112)\n",
                        "Function pad shape:  (539, 1, 112, 112)\n",
                        "Function pad shape:  (517, 1, 112, 112)\n",
                        "Function pad shape:  (541, 1, 112, 112)\n",
                        "Function pad shape:  (270, 1, 112, 112)\n",
                        "Function pad shape:  (513, 1, 112, 112)\n",
                        "Function pad shape:  (269, 1, 112, 112)\n",
                        "Function pad shape:  (1, 1, 112, 112)\n",
                        "Function pad shape:  (544, 1, 112, 112)\n",
                        "Function pad shape:  (151, 1, 112, 112)\n",
                        "Function pad shape:  (293, 1, 112, 112)\n",
                        "Function pad shape:  (527, 1, 112, 112)\n",
                        "Function pad shape:  (524, 1, 112, 112)\n",
                        "Function pad shape:  (502, 1, 112, 112)\n",
                        "Function pad shape:  (1, 1, 112, 112)\n",
                        "Function pad shape:  (539, 1, 112, 112)\n",
                        "Function pad shape:  (537, 1, 112, 112)\n",
                        "Function pad shape:  (536, 1, 112, 112)\n",
                        "Function pad shape:  (240, 1, 112, 112)\n",
                        "Function pad shape:  (518, 1, 112, 112)\n",
                        "Function pad shape:  (268, 1, 112, 112)\n",
                        "Function pad shape:  (906, 1, 112, 112)\n",
                        "Function pad shape:  (151, 1, 112, 112)\n",
                        "Function pad shape:  (517, 1, 112, 112)\n",
                        "Function pad shape:  (151, 1, 112, 112)\n",
                        "combined lens: 15201 4273\n",
                        "Function pad shape:  (534, 1, 112, 112)\n",
                        "combined sample: (8, 1, 112, 112) (2, 1, 112, 112)\n"
                    ]
                }
            ],
            "source": [
                "subject_to_cache_dir = {\n",
                "    \"A\": repo_root / \"derivatives\" / \"preprocessing\" / \"secundo\" / \"baseline_only\",\n",
                "    \"B\": repo_root / \"derivatives\" / \"preprocessing\" / \"gus\" / \"baseline_only\",\n",
                "}\n",
                "combined_manifest = create_split_manifest_multi(\n",
                "    subject_to_cache_dir=subject_to_cache_dir,\n",
                "    split_ratio=0.8,\n",
                "    seed=42,\n",
                "    pattern=\"baseline_*.npz\",\n",
                "    manifest_path=repo_root / \"derivatives\" / \"preprocessing\" / \"splits_multi.json\",\n",
                ")\n",
                "ds_train_combined = FUSForecastWindowDataset(\n",
                "    manifest_path=combined_manifest,\n",
                "    split=\"train\",\n",
                "    window_size=8,\n",
                "    pred_horizon=2,\n",
                "    stride=1,\n",
                ")\n",
                "ds_test_combined = FUSForecastWindowDataset(\n",
                "    manifest_path=combined_manifest,\n",
                "    split=\"test\",\n",
                "    window_size=8,\n",
                "    pred_horizon=2,\n",
                "    stride=1,\n",
                ")\n",
                "print(\"combined lens:\", len(ds_train_combined), len(ds_test_combined))\n",
                "x, y = ds_train_combined[0]\n",
                "print(\"combined sample:\", tuple(x.shape), tuple(y.shape))\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "basefus",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.11.14"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
