{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7b1f0c9",
   "metadata": {},
   "source": [
    "## dataset creation\n",
    "adapted from Leo's code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37da4888",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# Add repo root and code/ to sys.path\n",
    "repo_root = Path.cwd().parents[1]\n",
    "sys.path.insert(0, str(repo_root))\n",
    "sys.path.insert(0, str(repo_root / \"code\"))\n",
    "\n",
    "from utils import helper_functions as hf\n",
    "# Package import\n",
    "import scipy.io\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as TF\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from skimage.registration import phase_cross_correlation\n",
    "import os\n",
    "import time\n",
    "import importlib\n",
    "import torch.nn.functional as F\n",
    "import glob\n",
    "from scipy import signal\n",
    "import random\n",
    "import pickle\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b7fdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset creation class by leo\n",
    "\n",
    "def gaussian_kernel(size, sigma):\n",
    "    \"\"\"Generate a Gaussian kernel.\"\"\"\n",
    "    x = torch.arange(-size // 2 + 1, size // 2 + 1, dtype=torch.float32)  # Ensure float32\n",
    "    y = x\n",
    "    xx, yy = torch.meshgrid(x, y, indexing='ij')\n",
    "    kernel = torch.exp(-(xx**2 + yy**2) / (2 * sigma**2))\n",
    "    kernel /= kernel.sum()\n",
    "    return kernel\n",
    "    \n",
    "class GaussianNoise:\n",
    "    \"\"\"Add Gaussian noise to the tensor.\"\"\"\n",
    "    def __init__(self, mean=0., std=0.01):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        return tensor + torch.randn_like(tensor) * self.std + self.mean\n",
    "\n",
    "class FilteredFUSWindowDataset(Dataset):\n",
    "    def __init__(self, data_tensor, labels_tensor, window_size=8, stride=1, image_size=112, mode='train'):\n",
    "        \"\"\"\n",
    "        Custom Dataset for sliding windows of FUS frames with cropping and normalization.\n",
    "        For 'train' mode: Balances windows between labels 0 and 1 by downsampling majority class, and applies data augmentation.\n",
    "        For 'test' mode: Includes all windows, no balancing, no augmentation.\n",
    "        \n",
    "        Args:\n",
    "            data_tensor (torch.Tensor): Shape [N, 1, H, W] (e.g., [6000, 1, 112, 112]).\n",
    "            labels_tensor (torch.Tensor): Shape [N] with labels (0, 1). Assumes -1 already excluded.\n",
    "            window_size (int): Number of frames per window (e.g., 8).\n",
    "            stride (int): Step size for sliding window (e.g., 9 for non-overlapping with small gap).\n",
    "            image_size (int): Target square size for frames (e.g., 112).\n",
    "            mode (str): 'train' or 'test' to control balancing and augmentation.\n",
    "        \"\"\"\n",
    "        assert data_tensor.shape[0] == labels_tensor.shape[0], \"Data and labels length mismatch\"\n",
    "        assert window_size > 0, \"Window size must be positive\"\n",
    "        assert mode in ['train', 'test'], \"Mode must be 'train' or 'test'\"\n",
    "        \n",
    "        self.data = data_tensor\n",
    "        self.labels = labels_tensor\n",
    "        self.window_size = window_size\n",
    "        self.stride = stride\n",
    "        self.image_size = image_size\n",
    "        self.mode = mode\n",
    "        \n",
    "        self.transform = T.Compose([\n",
    "            T.Resize((image_size, image_size))  # Ensure square (redundant if already sized)\n",
    "        ])\n",
    "\n",
    "        # Build valid indices, skipping excluded zones\n",
    "        self.valid_indices = []\n",
    "        for i in range(window_size - 1, len(self.labels), stride):\n",
    "            if self.labels[i] != -1:  # Assume -1 already filtered out in preprocessing\n",
    "                self.valid_indices.append(i)\n",
    "\n",
    "        if len(self.valid_indices) == 0:\n",
    "            print(\"Warning: No valid windows found. Check labels or window_size.\")\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.valid_indices)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            end_idx = self.valid_indices[idx]\n",
    "            start_idx = end_idx - self.window_size + 1\n",
    "            if start_idx < 0 or end_idx >= len(self.data):\n",
    "                raise IndexError(f\"Invalid window range: [{start_idx}:{end_idx + 1}]\")\n",
    "            \n",
    "            window = self.data[start_idx:end_idx + 1]  # [T, 1, H, W]\n",
    "            H, W = window.shape[-2:]\n",
    "            window = torch.stack([self.transform(frame) for frame in window])  # [T, 1, 112, 112]\n",
    "            \n",
    "            # Verify window shape\n",
    "            if window.shape[0] != self.window_size:\n",
    "                raise ValueError(f\"Expected {self.window_size} frames, got {window.shape[0]}\")\n",
    "                \n",
    "            # === DATA AUGMENTATION (ONLY IN TRAIN MODE) ===\n",
    "            if self.mode == 'train':\n",
    "                # 1. Random Affine (same transformation for all frames)\n",
    "                affine = T.RandomAffine(degrees=30, translate=(0.2, 0.2), scale=(0.8, 1.2))\n",
    "                angle, translations, scale, shear = affine.get_params(\n",
    "                    affine.degrees, affine.translate, affine.scale, affine.shear,\n",
    "                    (self.image_size, self.image_size)\n",
    "                )\n",
    "                for t in range(window.size(0)):\n",
    "                    window[t] = TF.affine(\n",
    "                        window[t], angle=angle, translate=translations,\n",
    "                        scale=scale, shear=shear,\n",
    "                        interpolation=TF.InterpolationMode.BILINEAR, fill=0\n",
    "                    )\n",
    "                # 3. Gaussian noise (independent per frame)\n",
    "                noise = GaussianNoise(std=0.05)\n",
    "                window = noise(window)\n",
    "            \n",
    "            \n",
    "            label = self.labels[end_idx]\n",
    "            return window, label\n",
    "        except Exception as e:\n",
    "            print(f\"Error in __getitem__ at idx {idx}, end_idx {end_idx}: {e}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e0e51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "train_dataset = FilteredFUSWindowDataset(\n",
    "    data_tensor=train_images,\n",
    "    labels_tensor=train_labels,\n",
    "    window_size=window_size,\n",
    "    stride=stride,\n",
    "    image_size=image_size,\n",
    "    mode='train'\n",
    ")\n",
    "\n",
    "test_dataset = FilteredFUSWindowDataset(\n",
    "    data_tensor=test_images,\n",
    "    labels_tensor=test_labels,\n",
    "    window_size=window_size,\n",
    "    stride=stride,\n",
    "    image_size=image_size,\n",
    "    mode='test'\n",
    ")\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataset)} windows\")\n",
    "print(f\"Test dataset size: {len(test_dataset)} windows\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basefus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
