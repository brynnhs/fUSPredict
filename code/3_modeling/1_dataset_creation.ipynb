{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7b1f0c9",
   "metadata": {},
   "source": [
    "## dataset creation\n",
    "adapted from Leo's code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a80002",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# Third-party\n",
    "import numpy as np\n",
    "\n",
    "# Path setup \n",
    "repo_root = Path.cwd().parents[0]\n",
    "sys.path.insert(0, str(repo_root))\n",
    "sys.path.insert(0, str(repo_root / \"code\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401395fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.datasets import FUSForecastWindowDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfca860c",
   "metadata": {},
   "source": [
    "## find files + split-manifest utilities (single-subject, per-subject, multi-subject, held-out)\n",
    "\n",
    "the cell below has functions to find the cached baseline files and create different test/train splits. You can either split per subject, by combining all acquisitions for all subjects, or by training on one subject and testing on another to test for generalization. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b7fdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.manifest_utils import (\n",
    "    assert_manifest_paths_exist,\n",
    "    normalize_manifest_path,\n",
    "    normalize_manifest_payload,\n",
    ")\n",
    "\n",
    "\n",
    "def discover_acquisition_npz(cache_dir, pattern=\"baseline_*.npz\"):\n",
    "    \"\"\"Find one cached .npz per acquisition in one cache dir.\"\"\"\n",
    "    paths = sorted(glob.glob(os.path.join(str(cache_dir), pattern)))\n",
    "    if not paths:\n",
    "        raise FileNotFoundError(\n",
    "            f\"No acquisition caches found in {cache_dir} with pattern {pattern}\"\n",
    "        )\n",
    "    return [normalize_manifest_path(p) for p in paths]\n",
    "\n",
    "\n",
    "def discover_acquisitions_multi(subject_to_cache_dir, pattern=\"baseline_*.npz\"):\n",
    "    \"\"\"\n",
    "    find acquisitions across subjects.\n",
    "    Returns deterministic list: [{\"path\": str, \"subject\": str}, ...]\n",
    "    \"\"\"\n",
    "    if not isinstance(subject_to_cache_dir, dict) or len(subject_to_cache_dir) == 0:\n",
    "        raise ValueError(\"subject_to_cache_dir must be a non-empty dict\")\n",
    "    out = []\n",
    "    for subject in sorted(subject_to_cache_dir.keys()):\n",
    "        for p in discover_acquisition_npz(\n",
    "            subject_to_cache_dir[subject], pattern=pattern\n",
    "        ):\n",
    "            out.append({\"path\": p, \"subject\": str(subject)})\n",
    "    return out\n",
    "\n",
    "\n",
    "def _read_num_frames(npz_path, frames_key=\"frames\"):\n",
    "    with np.load(npz_path, allow_pickle=True) as z:\n",
    "        if frames_key not in z:\n",
    "            raise KeyError(f\"{npz_path} missing key '{frames_key}'\")\n",
    "        x = z[frames_key]\n",
    "        if x.ndim not in (3, 4):\n",
    "            raise ValueError(\n",
    "                f\"{npz_path} '{frames_key}' must be [T,H,W] or [T,C,H,W], got {x.shape}\"\n",
    "            )\n",
    "        return int(x.shape[0])\n",
    "\n",
    "\n",
    "def _count_by_subject(paths, path_to_subject):\n",
    "    return Counter(path_to_subject.get(p, \"UNK\") for p in paths)\n",
    "\n",
    "\n",
    "def _print_split_summary(train_paths, test_paths, path_to_subject, frames_key=\"frames\"):\n",
    "    train_frames = sum(_read_num_frames(p, frames_key=frames_key) for p in train_paths)\n",
    "    test_frames = sum(_read_num_frames(p, frames_key=frames_key) for p in test_paths)\n",
    "    tr_cnt = _count_by_subject(train_paths, path_to_subject)\n",
    "    te_cnt = _count_by_subject(test_paths, path_to_subject)\n",
    "    tr_txt = \", \".join([f\"{m}:{tr_cnt[m]}\" for m in sorted(tr_cnt.keys())])\n",
    "    te_txt = \", \".join([f\"{m}:{te_cnt[m]}\" for m in sorted(te_cnt.keys())])\n",
    "    print(\n",
    "        f\"Train acquisitions: {len(train_paths)} | total frames: {train_frames} | per-subject=({tr_txt})\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Test acquisitions:  {len(test_paths)} | total frames: {test_frames} | per-subject=({te_txt})\"\n",
    "    )\n",
    "\n",
    "\n",
    "def _write_manifest(manifest, manifest_path):\n",
    "    manifest_out = normalize_manifest_payload(manifest)\n",
    "    manifest_path = Path(manifest_path).expanduser().resolve(strict=False)\n",
    "    manifest_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with manifest_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(manifest_out, f, indent=2)\n",
    "    n_checked = assert_manifest_paths_exist(manifest_path)\n",
    "    print(\n",
    "        f\"Manifest saved: {manifest_path.as_posix()} \"\n",
    "        f\"(validated {n_checked} manifest path entries)\"\n",
    "    )\n",
    "    return manifest_path.as_posix()\n",
    "\n",
    "\n",
    "def create_split_manifest(\n",
    "    cache_dir,\n",
    "    split_ratio=0.8,\n",
    "    seed=42,\n",
    "    manifest_path=None,\n",
    "    pattern=\"baseline_*.npz\",\n",
    "    frames_key=\"frames\",\n",
    "):\n",
    "    \"\"\"Single-cache deterministic acquisition-wise split.\"\"\"\n",
    "    if not (0.0 < split_ratio < 1.0):\n",
    "        raise ValueError(\"split_ratio must be between 0 and 1\")\n",
    "    paths = discover_acquisition_npz(cache_dir, pattern=pattern)\n",
    "    rng = random.Random(seed)\n",
    "    shuffled = paths.copy()\n",
    "    rng.shuffle(shuffled)\n",
    "    n_total = len(shuffled)\n",
    "    n_train = int(split_ratio * n_total)\n",
    "    n_train = max(1, min(n_total - 1, n_train)) if n_total > 1 else 1\n",
    "    train_paths = sorted(shuffled[:n_train])\n",
    "    test_paths = sorted(shuffled[n_train:])\n",
    "    manifest = {\n",
    "        \"train\": train_paths,\n",
    "        \"test\": test_paths,\n",
    "        \"seed\": int(seed),\n",
    "        \"split_ratio\": float(split_ratio),\n",
    "        \"timestamp\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "        \"cache_dir\": str(cache_dir),\n",
    "        \"pattern\": pattern,\n",
    "        \"frames_key\": frames_key,\n",
    "    }\n",
    "    if manifest_path is None:\n",
    "        manifest_path = os.path.join(str(cache_dir), \"splits.json\")\n",
    "    out = _write_manifest(manifest, manifest_path)\n",
    "    path_to_subject = {p: Path(p).parent.name for p in train_paths + test_paths}\n",
    "    _print_split_summary(\n",
    "        train_paths, test_paths, path_to_subject, frames_key=frames_key\n",
    "    )\n",
    "    return out\n",
    "\n",
    "\n",
    "def create_split_manifest_multi(\n",
    "    subject_to_cache_dir,\n",
    "    split_ratio=0.8,\n",
    "    seed=42,\n",
    "    pattern=\"baseline_*.npz\",\n",
    "    frames_key=\"frames\",\n",
    "    manifest_path=None,\n",
    "):\n",
    "    \"\"\"Combined split across all subjects (acquisition-wise random, subject metadata preserved).\"\"\"\n",
    "    if not (0.0 < split_ratio < 1.0):\n",
    "        raise ValueError(\"split_ratio must be between 0 and 1\")\n",
    "    acqs = discover_acquisitions_multi(subject_to_cache_dir, pattern=pattern)\n",
    "    rng = random.Random(seed)\n",
    "    shuffled = acqs.copy()\n",
    "    rng.shuffle(shuffled)\n",
    "    n_total = len(shuffled)\n",
    "    n_train = int(split_ratio * n_total)\n",
    "    n_train = max(1, min(n_total - 1, n_train)) if n_total > 1 else 1\n",
    "    train_acqs = sorted(shuffled[:n_train], key=lambda d: d[\"path\"])\n",
    "    test_acqs = sorted(shuffled[n_train:], key=lambda d: d[\"path\"])\n",
    "    train_paths = [d[\"path\"] for d in train_acqs]\n",
    "    test_paths = [d[\"path\"] for d in test_acqs]\n",
    "    path_to_subject = {d[\"path\"]: d[\"subject\"] for d in acqs}\n",
    "    manifest = {\n",
    "        \"train\": train_paths,\n",
    "        \"test\": test_paths,\n",
    "        \"meta\": {p: {\"subject\": m} for p, m in path_to_subject.items()},\n",
    "        \"acquisitions\": acqs,\n",
    "        \"seed\": int(seed),\n",
    "        \"split_ratio\": float(split_ratio),\n",
    "        \"timestamp\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "        \"subject_to_cache_dir\": {\n",
    "            str(k): str(v) for k, v in subject_to_cache_dir.items()\n",
    "        },\n",
    "        \"pattern\": pattern,\n",
    "        \"frames_key\": frames_key,\n",
    "    }\n",
    "    if manifest_path is None:\n",
    "        first_dir = str(subject_to_cache_dir[sorted(subject_to_cache_dir.keys())[0]])\n",
    "        manifest_path = os.path.join(first_dir, \"splits_multi.json\")\n",
    "    out = _write_manifest(manifest, manifest_path)\n",
    "    _print_split_summary(\n",
    "        train_paths, test_paths, path_to_subject, frames_key=frames_key\n",
    "    )\n",
    "    return out\n",
    "\n",
    "\n",
    "def create_split_manifests_per_subject(\n",
    "    subject_to_cache_dir,\n",
    "    split_ratio=0.8,\n",
    "    seed=42,\n",
    "    pattern=\"baseline_*.npz\",\n",
    "    frames_key=\"frames\",\n",
    "    output_dir=None,\n",
    "):\n",
    "    \"\"\"Per-subject deterministic acquisition-wise split. Writes one manifest per subject.\"\"\"\n",
    "    if not isinstance(subject_to_cache_dir, dict) or len(subject_to_cache_dir) == 0:\n",
    "        raise ValueError(\"subject_to_cache_dir must be a non-empty dict\")\n",
    "    if not (0.0 < split_ratio < 1.0):\n",
    "        raise ValueError(\"split_ratio must be between 0 and 1\")\n",
    "    outputs = {}\n",
    "    for idx, subject in enumerate(sorted(subject_to_cache_dir.keys())):\n",
    "        cache_dir = subject_to_cache_dir[subject]\n",
    "        paths = discover_acquisition_npz(cache_dir, pattern=pattern)\n",
    "        rng = random.Random(int(seed) + idx)\n",
    "        shuffled = paths.copy()\n",
    "        rng.shuffle(shuffled)\n",
    "        n_total = len(shuffled)\n",
    "        n_train = int(split_ratio * n_total)\n",
    "        n_train = max(1, min(n_total - 1, n_train)) if n_total > 1 else 1\n",
    "        train_paths = sorted(shuffled[:n_train])\n",
    "        test_paths = sorted(shuffled[n_train:])\n",
    "        subject_str = str(subject)\n",
    "        path_to_subject = {p: subject_str for p in paths}\n",
    "        manifest = {\n",
    "            \"train\": train_paths,\n",
    "            \"test\": test_paths,\n",
    "            \"meta\": {p: {\"subject\": subject_str} for p in paths},\n",
    "            \"acquisitions\": [{\"path\": p, \"subject\": subject_str} for p in paths],\n",
    "            \"seed\": int(seed) + idx,\n",
    "            \"split_ratio\": float(split_ratio),\n",
    "            \"timestamp\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "            \"cache_dir\": str(cache_dir),\n",
    "            \"subject\": subject_str,\n",
    "            \"pattern\": pattern,\n",
    "            \"frames_key\": frames_key,\n",
    "        }\n",
    "        if output_dir is None:\n",
    "            manifest_path = os.path.join(str(cache_dir), f\"splits_{subject_str}.json\")\n",
    "        else:\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            manifest_path = os.path.join(str(output_dir), f\"splits_{subject_str}.json\")\n",
    "        out = _write_manifest(manifest, manifest_path)\n",
    "        print(f\"Summary for {subject_str} -> {Path(out).name}\")\n",
    "        _print_split_summary(\n",
    "            train_paths, test_paths, path_to_subject, frames_key=frames_key\n",
    "        )\n",
    "        outputs[subject_str] = out\n",
    "    return outputs\n",
    "\n",
    "\n",
    "def create_subject_heldout_manifests(\n",
    "    subject_to_cache_dir,\n",
    "    pattern=\"baseline_*.npz\",\n",
    "    frames_key=\"frames\",\n",
    "    output_dir=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    2-subject held-out split manifests:\n",
    "    - <A>_train_<B>_test.json\n",
    "    - <B>_train_<A>_test.json\n",
    "    \"\"\"\n",
    "    if not isinstance(subject_to_cache_dir, dict) or len(subject_to_cache_dir) != 2:\n",
    "        raise ValueError(\"subject_to_cache_dir must contain exactly 2 subjects\")\n",
    "    a, b = sorted(subject_to_cache_dir.keys())\n",
    "    a_acqs = discover_acquisitions_multi({a: subject_to_cache_dir[a]}, pattern=pattern)\n",
    "    b_acqs = discover_acquisitions_multi({b: subject_to_cache_dir[b]}, pattern=pattern)\n",
    "    a_paths = sorted([d[\"path\"] for d in a_acqs])\n",
    "    b_paths = sorted([d[\"path\"] for d in b_acqs])\n",
    "    path_to_subject = {d[\"path\"]: d[\"subject\"] for d in (a_acqs + b_acqs)}\n",
    "    meta = {p: {\"subject\": m} for p, m in path_to_subject.items()}\n",
    "    if output_dir is None:\n",
    "        output_dir = str(subject_to_cache_dir[a])\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    m1_path = os.path.join(output_dir, f\"{a}_train_{b}_test.json\")\n",
    "    m2_path = os.path.join(output_dir, f\"{b}_train_{a}_test.json\")\n",
    "    m1 = {\n",
    "        \"train\": a_paths,\n",
    "        \"test\": b_paths,\n",
    "        \"meta\": meta,\n",
    "        \"acquisitions\": a_acqs + b_acqs,\n",
    "        \"heldout_mode\": f\"train={a},test={b}\",\n",
    "        \"timestamp\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "        \"pattern\": pattern,\n",
    "        \"frames_key\": frames_key,\n",
    "    }\n",
    "    m2 = {\n",
    "        \"train\": b_paths,\n",
    "        \"test\": a_paths,\n",
    "        \"meta\": meta,\n",
    "        \"acquisitions\": a_acqs + b_acqs,\n",
    "        \"heldout_mode\": f\"train={b},test={a}\",\n",
    "        \"timestamp\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "        \"pattern\": pattern,\n",
    "        \"frames_key\": frames_key,\n",
    "    }\n",
    "    out1 = _write_manifest(m1, m1_path)\n",
    "    print(f\"Summary for {Path(out1).name}\")\n",
    "    _print_split_summary(a_paths, b_paths, path_to_subject, frames_key=frames_key)\n",
    "    out2 = _write_manifest(m2, m2_path)\n",
    "    print(f\"Summary for {Path(out2).name}\")\n",
    "    _print_split_summary(b_paths, a_paths, path_to_subject, frames_key=frames_key)\n",
    "    return out1, out2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d92cd05",
   "metadata": {},
   "source": [
    "## use it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74dc9ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mode = \"zscore\"\n",
    "if data_mode == \"raw\":\n",
    "    subject_to_cache_dir = {\n",
    "        \"A\": repo_root / \"derivatives\" / \"preprocessing\" / \"secundo\" / \"baseline_only\",\n",
    "        \"B\": repo_root / \"derivatives\" / \"preprocessing\" / \"gus\" / \"baseline_only\",\n",
    "    }\n",
    "elif data_mode in {\"mean_divide\", \"zscore\"}:\n",
    "    subject_to_cache_dir = {\n",
    "        \"A\": repo_root\n",
    "        / \"derivatives\"\n",
    "        / \"preprocessing\"\n",
    "        / \"secundo\"\n",
    "        / \"baseline_only_standardized\"\n",
    "        / data_mode,\n",
    "        \"B\": repo_root\n",
    "        / \"derivatives\"\n",
    "        / \"preprocessing\"\n",
    "        / \"gus\"\n",
    "        / \"baseline_only_standardized\"\n",
    "        / data_mode,\n",
    "    }\n",
    "else:\n",
    "    raise ValueError(f\"Unsupported data_mode: {data_mode}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cbabdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_manifest = create_split_manifest_multi(\n",
    "    subject_to_cache_dir=subject_to_cache_dir,\n",
    "    split_ratio=0.8,\n",
    "    seed=42,\n",
    "    pattern=\"baseline_*.npz\",\n",
    "    manifest_path=repo_root / \"derivatives\" / \"preprocessing\" / \"splits_multi.json\",\n",
    ")\n",
    "ds_train_combined = FUSForecastWindowDataset(\n",
    "    manifest_path=combined_manifest,\n",
    "    split=\"train\",\n",
    "    window_size=8,\n",
    "    pred_horizon=2,\n",
    "    stride=1,\n",
    ")\n",
    "ds_test_combined = FUSForecastWindowDataset(\n",
    "    manifest_path=combined_manifest,\n",
    "    split=\"test\",\n",
    "    window_size=8,\n",
    "    pred_horizon=2,\n",
    "    stride=1,\n",
    ")\n",
    "print(\"combined lens:\", len(ds_train_combined), len(ds_test_combined))\n",
    "x, y = ds_train_combined[0]\n",
    "print(\"combined sample:\", tuple(x.shape), tuple(y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b7be69",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_manifest = create_split_manifest(\n",
    "    cache_dir=repo_root\n",
    "    / \"derivatives\"\n",
    "    / \"preprocessing\"\n",
    "    / \"secundo\"\n",
    "    / \"baseline_only_standardized\"\n",
    "    / data_mode,\n",
    "    split_ratio=0.8,\n",
    "    seed=42,\n",
    "    pattern=\"baseline_*.npz\",\n",
    "    manifest_path=repo_root\n",
    "    / \"derivatives\"\n",
    "    / \"preprocessing\"\n",
    "    / \"splits_single_secundo.json\",\n",
    ")\n",
    "ds_train_single = FUSForecastWindowDataset(\n",
    "    manifest_path=single_manifest,\n",
    "    split=\"train\",\n",
    "    window_size=8,\n",
    "    pred_horizon=2,\n",
    "    stride=1,\n",
    ")\n",
    "ds_test_single = FUSForecastWindowDataset(\n",
    "    manifest_path=single_manifest,\n",
    "    split=\"test\",\n",
    "    window_size=8,\n",
    "    pred_horizon=2,\n",
    "    stride=1,\n",
    ")\n",
    "print(\"single lens:\", len(ds_train_single), len(ds_test_single))\n",
    "x, y = ds_train_single[0]\n",
    "print(\"single sample:\", tuple(x.shape), tuple(y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff45ffec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch128",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}